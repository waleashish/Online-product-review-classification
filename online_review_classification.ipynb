{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Review Sentiment Analysis for Product Analysis"
      ],
      "metadata": {
        "id": "WAXtrsBk4_Jd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mtqx0wqqyhDr",
        "outputId": "107a1db2-97cf-4e56-99d1-c9faed1da271"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras import layers\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "\n",
        "The dataset used is 'Flipkart Product reviews with sentiment Dataset'. It is publicly available to download at URL : https://www.kaggle.com/datasets/niraliivaghani/flipkart-product-customer-reviews-dataset\n",
        "\n",
        "\n",
        "In our case, the database is already present at the project's github repository inside 'data' folder."
      ],
      "metadata": {
        "id": "YXrqv-aL5FUz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"https://raw.githubusercontent.com/waleashish/online-product-review-classification/main/data/Dataset-SA.csv\",\n",
        "                 low_memory=False)"
      ],
      "metadata": {
        "id": "H00p8Qpu3onz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The online dataset contains many columns such as 'Product Name', 'Product Price', etc. We only need 'Sentiment' and 'Summary' for our modelling."
      ],
      "metadata": {
        "id": "kRvWAFEDkgx1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = df[['Summary', 'Sentiment']]"
      ],
      "metadata": {
        "id": "NhkgyOtT4Fd0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This block contains all the helper functions\n",
        "\n",
        "\"\"\"\n",
        "    This function performs clearing process on the input text.\n",
        "    Arguments:\n",
        "      text : sentence to be processed\n",
        "    Returns:\n",
        "      The clean sentence\n",
        "\"\"\"\n",
        "def process_sentence(text):\n",
        "    stemmer = PorterStemmer()\n",
        "    stopwords_english = stopwords.words('english')\n",
        "    # remove unwanted text patterns (hyperlinks, hashtags, etc.)\n",
        "    text = re.sub(r'\\$\\w*', '', str(text))\n",
        "    # remove old style retweet text \"RT\"\n",
        "    text = re.sub(r'^RT[\\s]+', '', str(text))\n",
        "    # remove hyperlinks\n",
        "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', str(text))\n",
        "    # remove hashtags\n",
        "    # only removing the hash # sign from the word\n",
        "    text = re.sub(r'#', '', str(text))\n",
        "    # tokenize tweets\n",
        "    text_tokens = word_tokenize(text)\n",
        "    # make all word tokens lower case\n",
        "    text_tokens = [token.lower() for token in text_tokens]\n",
        "    # remove punctuation\n",
        "    text_clean = \"\"\n",
        "    for word in text_tokens:\n",
        "        if word not in stopwords_english and word not in string.punctuation:\n",
        "            text_clean += stemmer.stem(word) + \" \"\n",
        "\n",
        "    return text_clean[:-1]"
      ],
      "metadata": {
        "id": "s1TFt8v7HQcA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Our 'Sentiment' column has sentiments as strings. We need to encode them into integers. We'll use sklearn's `LabelEncoder` for this purpose.\n",
        "\n",
        "> Our 'Summary' column, which contains the reviews, has original reviews. We need to preprocess them to remove punctuations, stopwords, etc."
      ],
      "metadata": {
        "id": "slou8t29HqT7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "le = LabelEncoder()\n",
        "# Encode the sentiments as integers\n",
        "dataset['label'] = le.fit_transform(df['Sentiment'])\n",
        "# Pre process the reviews\n",
        "dataset['Summary'] = dataset['Summary'].apply(lambda x : process_sentence(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7y_un_u_JRZ",
        "outputId": "2190dc5d-faa8-43d4-e7b0-2286be2f5f12"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-105647d671c9>:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dataset['label'] = le.fit_transform(df['Sentiment'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the training, testing and validation datasets."
      ],
      "metadata": {
        "id": "9SPIqc_7HlxW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training and testing data\n",
        "train_sentences, temp_test_sentences, train_labels, temp_test_labels = train_test_split(dataset['Summary'],\n",
        "                                                                                        dataset['label'],\n",
        "                                                                                        test_size=0.2)\n",
        "\n",
        "# Create validation and testing data\n",
        "valid_sentences, test_sentences, valid_labels, test_labels = train_test_split(temp_test_sentences,\n",
        "                                                                              temp_test_labels,\n",
        "                                                                              test_size=0.5)"
      ],
      "metadata": {
        "id": "xSXwiwG65kFp"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model\n",
        "\n",
        "We are creating following model architecture:\n",
        "\n",
        "\n",
        "1.   Text Vectorization Layer\n",
        "2.   Embedding Layer\n",
        "3.   LSTM Layers\n",
        "4.   Dense Layers\n",
        "5    Output Dense Layer\n",
        "\n",
        "\n",
        "Our compilation parameters:\n",
        "\n",
        "1.   Optimizer: Adam\n",
        "2.   Loss: Sparse Categorical Cross Entropy\n",
        "3.   Metric: Accuracy\n",
        "\n",
        "\n",
        "Our Fitting parameters:\n",
        "\n",
        "1.   Epochs: 10\n",
        "\n"
      ],
      "metadata": {
        "id": "Y0NooiPzAIwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Declare variables\n",
        "MAX_VOCAB_SIZE = 10000\n",
        "MAXLEN = 25\n",
        "\n",
        "# Create text vectorization layers\n",
        "text_vectorizer = layers.TextVectorization(max_tokens=MAX_VOCAB_SIZE,\n",
        "                                           output_mode='int',\n",
        "                                           output_sequence_length=MAXLEN)\n",
        "text_vectorizer.adapt(train_sentences)\n",
        "# Create embedding layer\n",
        "embedding = layers.Embedding(input_dim=MAX_VOCAB_SIZE,\n",
        "                             output_dim=128,\n",
        "                             input_length=MAXLEN)"
      ],
      "metadata": {
        "id": "wrJYedsoAYNo"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model\n",
        "inputs = layers.Input((1, ), dtype='string')\n",
        "x = text_vectorizer(inputs)\n",
        "x = embedding(x)\n",
        "x = layers.LSTM(64)(x)\n",
        "x =\n",
        "outputs = layers.Dense(3, activation='softmax')(x)\n",
        "model = tf.keras.Model(inputs, outputs, name=\"review_sentiment_analysis\")\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              optimizer=tf.keras.optimizers.Adam(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Fit the model\n",
        "history = model.fit(x=train_sentences,\n",
        "                    y=train_labels,\n",
        "                    epochs=10,\n",
        "                    validation_data=(valid_sentences, valid_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmWGrjGZQEy6",
        "outputId": "23460037-52ae-4b6c-cf7e-27d11066d749"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "5127/5127 [==============================] - 87s 16ms/step - loss: 0.2281 - accuracy: 0.9292 - val_loss: 0.2638 - val_accuracy: 0.9192\n",
            "Epoch 2/10\n",
            "5127/5127 [==============================] - 60s 12ms/step - loss: 0.1802 - accuracy: 0.9416 - val_loss: 0.2705 - val_accuracy: 0.9170\n",
            "Epoch 3/10\n",
            "5127/5127 [==============================] - 56s 11ms/step - loss: 0.1657 - accuracy: 0.9469 - val_loss: 0.2797 - val_accuracy: 0.9184\n",
            "Epoch 4/10\n",
            "5127/5127 [==============================] - 56s 11ms/step - loss: 0.1560 - accuracy: 0.9498 - val_loss: 0.2991 - val_accuracy: 0.9159\n",
            "Epoch 5/10\n",
            "5127/5127 [==============================] - 67s 13ms/step - loss: 0.1500 - accuracy: 0.9517 - val_loss: 0.3033 - val_accuracy: 0.9167\n",
            "Epoch 6/10\n",
            "5127/5127 [==============================] - 67s 13ms/step - loss: 0.1434 - accuracy: 0.9539 - val_loss: 0.3220 - val_accuracy: 0.9144\n",
            "Epoch 7/10\n",
            "5127/5127 [==============================] - 63s 12ms/step - loss: 0.1393 - accuracy: 0.9553 - val_loss: 0.3170 - val_accuracy: 0.9141\n",
            "Epoch 8/10\n",
            "5127/5127 [==============================] - 62s 12ms/step - loss: 0.1349 - accuracy: 0.9567 - val_loss: 0.3266 - val_accuracy: 0.9187\n",
            "Epoch 9/10\n",
            "5127/5127 [==============================] - 62s 12ms/step - loss: 0.1318 - accuracy: 0.9574 - val_loss: 0.3358 - val_accuracy: 0.9136\n",
            "Epoch 10/10\n",
            "5127/5127 [==============================] - 58s 11ms/step - loss: 0.1290 - accuracy: 0.9583 - val_loss: 0.3612 - val_accuracy: 0.9135\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing evaluation"
      ],
      "metadata": {
        "id": "ms_qr0F8_Lxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_sentences, test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hX92q3Lq_PTs",
        "outputId": "08458872-0255-441a-c7b5-27c6fb939381"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "641/641 [==============================] - 4s 6ms/step - loss: 0.3605 - accuracy: 0.9110\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.36045315861701965, 0.9110016822814941]"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction on new data"
      ],
      "metadata": {
        "id": "FfyV-3ZafgiV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentiment(model, sentences):\n",
        "    # Get prediction probabilities\n",
        "    pred_probs = model.predict(sentences)\n",
        "\n",
        "    # Get the index of maximum probability, that is our label\n",
        "    label = np.argmax(pred_probs, axis=-1)\n",
        "\n",
        "    # Convert the label back to sentiment and return it\n",
        "    return list(le.inverse_transform(label))"
      ],
      "metadata": {
        "id": "9_Vy72kzgU0o"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent = [\n",
        "    \"Wow!!! Its the very first word that i said as i put on this headset. Amazing Product, and Delivery time was also great.\",\n",
        "    \"An amazing experience. Never seen never dreamt experiences. A worthy entertainer. Cost could have been more affordable.\",\n",
        "    \"It was a pretty decent set up & fun to play games on as well.\",\n",
        "    \"Item is defective. Contacted the customer service number and email provided by the Amazon customer service.\",\n",
        "    \"Please take back product because of not working properly.\"\n",
        "    ]\n",
        "\n",
        "predict_sentiment(model, sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_B8BV8vWDTd",
        "outputId": "d6f87606-cc5b-4cd3-e14d-f826a3784b0c"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 39ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['positive', 'neutral', 'positive', 'negative', 'negative']"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    }
  ]
}