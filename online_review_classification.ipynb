{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Review Sentiment Analysis for Product Analysis"
      ],
      "metadata": {
        "id": "WAXtrsBk4_Jd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mtqx0wqqyhDr",
        "outputId": "7babe1ba-c0b2-42fc-a093-c7b4867abb08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras import layers\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "\n",
        "The dataset used is 'Flipkart Product reviews with sentiment Dataset'. It is publicly available to download at URL : https://www.kaggle.com/datasets/niraliivaghani/flipkart-product-customer-reviews-dataset\n",
        "\n",
        "\n",
        "In our case, the database is already present at the project's github repository inside 'data' folder."
      ],
      "metadata": {
        "id": "YXrqv-aL5FUz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"https://raw.githubusercontent.com/waleashish/online-product-review-classification/main/data/Dataset-SA.csv\",\n",
        "                 low_memory=False)"
      ],
      "metadata": {
        "id": "H00p8Qpu3onz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The online dataset contains many columns such as 'Product Name', 'Product Price', etc. We only need 'Sentiment' and 'Summary' for our modelling."
      ],
      "metadata": {
        "id": "kRvWAFEDkgx1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = df[['Summary', 'Sentiment']]"
      ],
      "metadata": {
        "id": "NhkgyOtT4Fd0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This block contains all the helper functions\n",
        "\n",
        "\"\"\"\n",
        "    This function performs clearing process on the input text.\n",
        "    Arguments:\n",
        "      text : sentence to be processed\n",
        "    Returns:\n",
        "      The clean sentence\n",
        "\"\"\"\n",
        "def process_sentence(text):\n",
        "    stemmer = PorterStemmer()\n",
        "    stopwords_english = stopwords.words('english')\n",
        "    # remove unwanted text patterns (hyperlinks, hashtags, etc.)\n",
        "    text = re.sub(r'\\$\\w*', '', str(text))\n",
        "    # remove old style retweet text \"RT\"\n",
        "    text = re.sub(r'^RT[\\s]+', '', str(text))\n",
        "    # remove hyperlinks\n",
        "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', str(text))\n",
        "    # remove hashtags\n",
        "    # only removing the hash # sign from the word\n",
        "    text = re.sub(r'#', '', str(text))\n",
        "    # tokenize tweets\n",
        "    text_tokens = word_tokenize(text)\n",
        "    # make all word tokens lower case\n",
        "    text_tokens = [token.lower() for token in text_tokens]\n",
        "    # remove punctuation\n",
        "    text_clean = \"\"\n",
        "    for word in text_tokens:\n",
        "        if word not in stopwords_english and word not in string.punctuation:\n",
        "            text_clean += stemmer.stem(word) + \" \"\n",
        "\n",
        "    return text_clean[:-1]"
      ],
      "metadata": {
        "id": "s1TFt8v7HQcA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Our 'Sentiment' column has sentiments as strings. We need to encode them into integers. We'll use sklearn's `LabelEncoder` for this purpose.\n",
        "\n",
        "> Our 'Summary' column, which contains the reviews, has original reviews. We need to preprocess them to remove punctuations, stopwords, etc."
      ],
      "metadata": {
        "id": "slou8t29HqT7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "le = LabelEncoder()\n",
        "# Encode the sentiments as integers\n",
        "dataset['label'] = le.fit_transform(df['Sentiment'])\n",
        "# Pre process the reviews\n",
        "dataset['Summary'] = dataset['Summary'].apply(lambda x : process_sentence(x))"
      ],
      "metadata": {
        "id": "N7y_un_u_JRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the training, testing and validation datasets."
      ],
      "metadata": {
        "id": "9SPIqc_7HlxW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training and testing data\n",
        "train_sentences, temp_test_sentences, train_labels, temp_test_labels = train_test_split(dataset['Summary'],\n",
        "                                                                                        dataset['label'],\n",
        "                                                                                        test_size=0.2)\n",
        "\n",
        "# Create validation and testing data\n",
        "valid_sentences, test_sentences, valid_labels, test_labels = train_test_split(temp_test_sentences,\n",
        "                                                                              temp_test_labels,\n",
        "                                                                              test_size=0.5)"
      ],
      "metadata": {
        "id": "xSXwiwG65kFp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model\n",
        "\n",
        "We are creating following model architecture:\n",
        "\n",
        "\n",
        "1.   Text Vectorization Layer\n",
        "2.   Embedding Layer\n",
        "3.   LSTM Layers\n",
        "4.   Dense Layers\n",
        "5    Output Dense Layer\n",
        "\n",
        "\n",
        "Our compilation parameters:\n",
        "\n",
        "1.   Optimizer: Adam\n",
        "2.   Loss: Sparse Categorical Cross Entropy\n",
        "3.   Metric: Accuracy\n",
        "\n",
        "\n",
        "Our Fitting parameters:\n",
        "\n",
        "1.   Epochs: 10\n",
        "\n"
      ],
      "metadata": {
        "id": "Y0NooiPzAIwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Declare variables\n",
        "MAX_VOCAB_SIZE = 10000\n",
        "MAXLEN = 25\n",
        "\n",
        "# Create text vectorization layers\n",
        "text_vectorizer = layers.TextVectorization(max_tokens=MAX_VOCAB_SIZE,\n",
        "                                           output_mode='int',\n",
        "                                           output_sequence_length=MAXLEN)\n",
        "text_vectorizer.adapt(train_sentences)\n",
        "# Create embedding layer\n",
        "embedding = layers.Embedding(input_dim=MAX_VOCAB_SIZE,\n",
        "                             output_dim=128,\n",
        "                             input_length=MAXLEN)"
      ],
      "metadata": {
        "id": "wrJYedsoAYNo"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model\n",
        "inputs = tf.keras.layers.Input(shape=(1,), dtype='string')\n",
        "x = text_vectorizer(inputs)\n",
        "x = embedding(x)\n",
        "x = layers.LSTM(64)(x)\n",
        "outputs = layers.Dense(3, activation='softmax')(x)\n",
        "model = tf.keras.Model(inputs, outputs, name=\"review_sentiment_analysis\")\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              optimizer=tf.keras.optimizers.Adam(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Fit the model\n",
        "history = model.fit(x=train_sentences,\n",
        "                    y=train_labels,\n",
        "                    epochs=10,\n",
        "                    validation_data=(valid_sentences, valid_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmWGrjGZQEy6",
        "outputId": "97f4ba05-3ca6-4f91-d038-ebe19f271f46"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "5127/5127 [==============================] - 70s 13ms/step - loss: 0.2927 - accuracy: 0.9072 - val_loss: 0.2535 - val_accuracy: 0.9213\n",
            "Epoch 2/10\n",
            "5127/5127 [==============================] - 44s 9ms/step - loss: 0.2375 - accuracy: 0.9237 - val_loss: 0.2377 - val_accuracy: 0.9239\n",
            "Epoch 3/10\n",
            "5127/5127 [==============================] - 44s 8ms/step - loss: 0.2137 - accuracy: 0.9301 - val_loss: 0.2333 - val_accuracy: 0.9232\n",
            "Epoch 4/10\n",
            "5127/5127 [==============================] - 43s 8ms/step - loss: 0.1973 - accuracy: 0.9343 - val_loss: 0.2416 - val_accuracy: 0.9234\n",
            "Epoch 5/10\n",
            "5127/5127 [==============================] - 43s 8ms/step - loss: 0.1841 - accuracy: 0.9393 - val_loss: 0.2486 - val_accuracy: 0.9205\n",
            "Epoch 6/10\n",
            "5127/5127 [==============================] - 44s 9ms/step - loss: 0.1722 - accuracy: 0.9433 - val_loss: 0.2567 - val_accuracy: 0.9211\n",
            "Epoch 7/10\n",
            "5127/5127 [==============================] - 42s 8ms/step - loss: 0.1626 - accuracy: 0.9462 - val_loss: 0.2666 - val_accuracy: 0.9216\n",
            "Epoch 8/10\n",
            "5127/5127 [==============================] - 43s 8ms/step - loss: 0.1537 - accuracy: 0.9497 - val_loss: 0.2799 - val_accuracy: 0.9165\n",
            "Epoch 9/10\n",
            "5127/5127 [==============================] - 42s 8ms/step - loss: 0.1472 - accuracy: 0.9519 - val_loss: 0.2873 - val_accuracy: 0.9207\n",
            "Epoch 10/10\n",
            "5127/5127 [==============================] - 47s 9ms/step - loss: 0.1410 - accuracy: 0.9540 - val_loss: 0.2952 - val_accuracy: 0.9187\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing evaluation"
      ],
      "metadata": {
        "id": "ms_qr0F8_Lxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_sentences, test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hX92q3Lq_PTs",
        "outputId": "409141ba-a2a4-4666-9a51-1188d54bfde2"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "641/641 [==============================] - 3s 4ms/step - loss: 0.3136 - accuracy: 0.9148\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.31364279985427856, 0.9148054122924805]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction on new data"
      ],
      "metadata": {
        "id": "FfyV-3ZafgiV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentiment(model, sentences):\n",
        "    # Get prediction probabilities\n",
        "    pred_probs = model.predict(sentences)\n",
        "\n",
        "    # Get the index of maximum probability, that is our label\n",
        "    label = np.argmax(pred_probs, axis=-1)\n",
        "\n",
        "    # Convert the label back to sentiment and return it\n",
        "    return list(le.inverse_transform(label))"
      ],
      "metadata": {
        "id": "9_Vy72kzgU0o"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent = [\n",
        "    \"Wow!!! Its the very first word that i said as i put on this headset. Amazing Product, and Delivery time was also great.\",\n",
        "    \"Please take back product because of not working properly.\",\n",
        "    \"An amazing experience. Never seen never dreamt experiences. A worthy entertainer. Cost could have been more affordable.\",\n",
        "    \"It was a pretty decent set up & fun to play games on as well.\",\n",
        "    \"Useless item!\"\n",
        "    ]\n",
        "\n",
        "predict_sentiment(model, sent)"
      ],
      "metadata": {
        "id": "n_B8BV8vWDTd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fd2197d-687b-480a-df68-23d9bf1948f2"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 36ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['positive', 'negative', 'positive', 'positive', 'negative']"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    }
  ]
}